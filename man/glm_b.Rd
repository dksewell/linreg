% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glm_b.R
\name{glm_b}
\alias{glm_b}
\title{Bayesian Generalized Linear Models}
\usage{
glm_b(
  formula,
  data,
  family,
  trials,
  prior = c("zellner", "normal", "improper")[1],
  zellner_g,
  prior_beta_mean,
  prior_beta_precision,
  ROPE,
  CI_level = 0.95,
  vb_maximum_iterations = 1000,
  algorithm = "VB",
  proposal_df = 5,
  seed = 1,
  mc_error = 0.01,
  save_memory = FALSE
)
}
\arguments{
\item{formula}{A formula specifying the model.}

\item{data}{A data frame in which the variables specified in the formula
will be found. If missing, the variables are searched for in the standard way.
However, it is strongly recommended that you use this argument so that other
generics for bayesics objects work correctly.}

\item{family}{A description of the error distribution and link function
to be used in the model. See ?glm for more information.}

\item{trials}{Integer vector giving the number of trials for each
observation if family = binomial().}

\item{prior}{character.  One of "zellner", "normal", or "improper", giving
the type of prior used on the regression coefficients.}

\item{zellner_g}{numeric.  Positive number giving the value of "g" in Zellner's
g prior.  Ignored unless prior = "zellner". Default is the number of observations.}

\item{prior_beta_mean}{numeric vector of same length as regression coefficients
(denoted p). Unless otherwise specified, automatically set to rep(0,p).  Ignored
unless prior = "normal".}

\item{prior_beta_precision}{pxp matrix giving a priori precision matrix to be
scaled by the residual precision.  Ignored
unless prior = "normal".}

\item{ROPE}{vector of positive values giving ROPE boundaries for each regression
coefficient.  Optionally, you can not include a ROPE boundary for the intercept.
If missing, defaults go to those suggested by Kruchke (2018).}

\item{CI_level}{numeric. Credible interval level.}

\item{vb_maximum_iterations}{if \code{algorithm = "VB"}, the number of
iterations used in the fixed-form VB algorithm.}

\item{algorithm}{Either "VB" (default) for fixed-form variational Bayes,
"IS" for importance sampling, or "LSA" for large sample approximation.}

\item{proposal_df}{degrees of freedom used in the multivariate t proposal
distribution if \code{algorithm = "IS"}.}

\item{seed}{integer.  Always set your seed!!!  Not used for
\code{algorithm = LSA}.}

\item{mc_error}{If importance sampling is used, the number of posterior
draws will ensure that with 99\% probability the bounds of the credible
intervals will be within \eqn{\pm} \code{mc_error}.}

\item{save_memory}{logical.  If TRUE, a more memory efficient approach
will be taken at the expense of computataional time (for important
sampling only.  But if memory is an issue, it's probably because you have a
large sample size, in which case the normal approximation sans IS should
probably work.)}
}
\value{
glm_b() returns an object of class "glm_b", which behaves as a list with
the following elements:
\itemize{
\item summary - tibble giving results for regression coefficients
\item posterior_draws - list giving the posterior parameters
\item hyperparms - list giving the user input (or default) hyperparameters used
\item fitted - posterior mean of the individuals' means
\item residuals - posterior mean of the residuals
\item formula, data - input by user
}

\strong{Importance sampling:}

\code{glm_b} will, unless \code{use_importance_sampling = FALSE}, perform importance sampling.
The proposal will use a multivariate t distribution, centered at the
posterior mode, with the negative hessian as its precision matrix.  Do NOT
treat the proposal_draws as posterior draws.

\strong{Priors:}

If the prior is set to be either "zellner" or "normal", a normal distribution
will be used as the prior of \eqn{\beta}, specifically
\deqn{\beta \sim N(\mu, V)}
where \eqn{\mu} is the prior_beta_mean and V is the prior_beta_precision (not covariance) matrix.
\itemize{
\item{\code{zellner}: \code{glm_b} sets \eqn{\mu=0} and \eqn{V = \frac{1}{g} X^{\top} X}.}
\item{\code{normal}: If missing \code{prior_beta_mean}, \code{glm_b} sets \eqn{\mu=0},
and if missing \code{prior_beta_precision} V will be a diagonal matrix.  The first element,
corresponding to the intercept, will be \eqn{(2.5\times \max{\tilde{s}_y,1})^{-2}}, where
\eqn{\tilde{s}_y} is max of 1 and the standard deviation of \eqn{y}.  Remaining diagonal elements
will equal \eqn{(2.5 s_y/s_x)^{-2}}, where \eqn{s_x} is the standard deviations
of the covariates.  This equates to being 95\% certain a priori that a change in
x by one standard deviation (\eqn{s_x}) would not lead to a change in the linear predictor of
more than 5 standard deviations (\eqn{5s_y}).  This imposes weak regularization that adapts to the scale
of the data elements.}
}

\strong{ROPE:}

If missing, the ROPE bounds will be given under the principle of "half of a
small effect size."
\itemize{
\item{Gaussian.  Using Cohen's D of 0.2 as a small effect size, the ROPE is
built under the principle that moving the full range of X (i.e., \eqn{\pm 2 s_x})
will not move the mean of y by more than the overall mean of \eqn{y}
minus \eqn{0.1s_y} to the overall mean of \eqn{y} plus \eqn{0.1s_y}.
The result is a ROPE equal to \eqn{|\beta_j| < 0.05s_y/s_j}.  If the covariate is
binary, then this is simply \eqn{|\beta_j| < 0.2s_y}.}
\item{Poisson. FDA guidance suggests a small effect is a rate ratio less
than 1.25.  We use half this effect: 1.125, and consider ROPE to indicate
that a moving the full range of X (\eqn{\pm 2s_x} will not change the rate
ratio by more than this amount.  Thus the ROPE for the regression
coefficient equals \eqn{|\beta| < \frac{\log(1.125)}{4s_x}}. For binary
covariates, this is simply \eqn{|\beta| < \log(1.125)}.}
}
}
\description{
glm_b is used to fit linear models.  It can be used to carry out
regression for gaussian, binomial, and poisson data.  Note that if
the family is gaussian, this is just a wrapper for \code{lm_b}.
}
\references{
Kruschke JK. Rejecting or Accepting Parameter Values in Bayesian Estimation. Advances in Methods and Practices in Psychological Science. 2018;1(2):270-280. doi:10.1177/2515245918771304

Tim Salimans. David A. Knowles. "Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression." Bayesian Anal. 8 (4) 837 - 882, December 2013. https://doi.org/10.1214/13-BA858
}
