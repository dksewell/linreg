---
title: "Introduction to bayesics"
subtitle: "bayesics Package Version `r packageVersion('bayesics')`"
format: 
  html:
    toc: true
    html-math-method: mathjax
    self-contained: true
vignette: >
  %\VignetteIndexEntry{Introduction-to-bayesics}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
bibliography: bibliography.bib
---

```{r, include = FALSE}

knitr::opts_chunk$set(
  strip.white = FALSE,
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 4,
  fig.align = "center",
  cache = TRUE
)

options(knitr.table.format = "html")
library(bayesics)
set.seed(2025)
```

# Introduction

The overarching goal of the **bayesics** R package is to provide a Bayesian implementation of the most commonly used statistical procedures in such a way that requires as little user input as possible into the algorithms behind the procedures.  That is, while we believe it is critical that users understand the *statistics* of the procedures, it is unessential that the *algorithms* are as well understood provided they yield the correct statistical inference. 


# Inference

The `bayesics` R package aims to provide the following inferential elements.

* Point and interval estimation is provided by each algorithm.  `summary()` functions allow the credible interval width to change (e.g., from 95% to 99%).
* Probability of direction is given (when applicable).  This is a fundamental quantity in Bayesian inference, telling us how sure we are of the direction of the effect.
* Region of Practical Equivalence, or ROPE, has long been argued to be of considerably more import than testing point null hypotheses, because
  a. We never really expect the truth to be *exactly* equal to the point null, thereby making the hypothesis test more about sample size, and 
  a. Decisions and scientific conclusions ought to depend heavily on effect size.
  The functions in `bayesics` provides the posterior probability that the estimand of interest (e.g., $\mu_1 - \mu_2$) lies within the ROPE.  Thanks to the nice guidelines in [@kruschke2018rejecting], reasonable default ROPE settings have been implemented on the principle of "Rope = half a small effect size."
* Despite the benefits to using ROPE over point nulls, the latter remain firmly entrenched in science and scientific reporting.  To accommodate this, Bayes factors have been provided in many functions, along with the interpretation from [@kass1995bayes].


# Algorithms

As stated above, we wish the users to focus on statistical inference, rather than the algorithms used.  Towards this, we have avoided MCMC in order to avoid the requirement of examining trace plots, testing for chain convergence, etc.  Instead, we have provided closed form solutions whenever possible, including of Bayes factors. 

In cases where sampling is unavoidable, we use independent posterior samples.  The number of these samples is determined automatically to obtain precise **credible interval** estimates.  This is a critical difference from most approaches, which aim to obtain precise **posterior mean** estimates.  However, decisions/conclusions are in practice made based on these tail quantiles (e.g., does the CI include zero?), and these tail quantiles require considerably more posterior samples than obtaining point estimates through the posterior mean.

As an example, consider the widely used R package `rstanarm` [@goodrich2025rstanarm].  While this package is of very high quality and is quite useful to experienced Bayesian practitioners implementing regression methods, the default number of MCMC iterations called is much too small to accurately estimate the CI bounds.  To illustrate, @fig-stan_glm_bound_errors shows the percent error of the lower and upper bounds of 95% and 99% credible intervals for the slope in a simple linear regression given by 
$$
  y_i = 0 + 0.25x_i + N(0,1).
$$
As can be seen from the figure, the estimated CI bounds typically range between $\pm10$% of the overall estimated CI bounds. 


```{r results = "hide"}
library(rstanarm)

# Generate toy dataset for simple linear regression (slr)
set.seed(2025)
N = 25
slr_data = 
  data.frame(x = rnorm(N))
slr_data$y = 
  0.25 * slr_data$x + rnorm(N)

# Use rstanarm to generate MCMC samples from the slr
stan_glm_default_fits = 
  lapply(1:250,
         function(i){
           set.seed(i)
           stan_glm(y ~ x,
                    slr_data,
                    family = gaussian())
           })
```


```{r}
#| label: fig-stan_glm_bound_errors
#| layout-ncol: 2
#| fig-cap: "Relative error (%) for upper and lower bounds of the credible intervals"
#| fig-subcap: 
#|   - "95% CI"
#|   - "99% CI"

library(ggplot2)

# Look at 95% CI
sapply(stan_glm_default_fits,
       function(object){
         posterior_interval(object,0.95)[2,]
       }) |> 
  t() |> 
  as.data.frame() |> 
  dplyr::mutate(`2.5%` = 100 * (1.0 - `2.5%` / mean(`2.5%`) )) |> 
  dplyr::mutate(`97.5%` = 100 * (1.0 - `97.5%` / mean(`97.5%`) )) |> 
  tidyr::pivot_longer(cols = everything(),
                      names_to = "CI bound",
                      values_to = "value") |> 
  ggplot(aes(x = value,
             fill = `CI bound`)) +
  geom_histogram() +
  scale_fill_viridis_d() +
  ylab("") +
  xlab("") +
  theme_minimal(base_size = 20)


# Look at 99% CI
sapply(stan_glm_default_fits,
       function(object){
         posterior_interval(object,0.99)[2,]
       }) |> 
  t() |> 
  as.data.frame() |> 
  dplyr::mutate(`0.5%` = 100 * (1.0 - `0.5%` / mean(`0.5%`) )) |> 
  dplyr::mutate(`99.5%` = 100 * (1.0 - `99.5%` / mean(`99.5%`) )) |> 
  tidyr::pivot_longer(cols = everything(),
                      names_to = "CI bound",
                      values_to = "value") |> 
  ggplot(aes(x = value,
             fill = `CI bound`)) +
  geom_histogram() +
  scale_fill_viridis_d() +
  ylab("") +
  xlab("") +
  theme_minimal(base_size = 20)
```


To ensure we estimate these bounds accurately, we may rely on work by [@doss2014markov] who provide a central limit theorem for order statistics, restated as follows: For a parameter of interest $\theta$, let $\hat\theta_{L,p}$ denote the $p^{th}$ empirical quantile of a  $iid$ sample of size $L$ (note that $L$ represents the number of posterior samples, not the sample size of the original data), let $\theta_{p}$ denote the corresponding true posterior quantile, and let $\pi()$ denote the posterior probability density function of $\theta|y$.  Then 
$$
  \sqrt{L}(\hat\theta_{L,p} - \theta_{p}) \overset{d}{\to} N\left(0,\frac{p(1-p)}{\pi^2(\theta_p)}\right).
$$
Hence, to estimate, say, the lower bound of a $(1-\alpha)100$% CI, i.e., the $(\alpha/2)^{th}$ posterior quantile of $\theta|y$, up to $\pm$ some error $\epsilon$ with probability $s$, i.e.,
$$
s = \Pr(\theta_{\frac{\alpha}{2}} - \epsilon < \hat\theta_{L,\frac{\alpha}{2}} < \theta_{\frac{\alpha}{2}} + \epsilon),
$$
we set $L$ to be
$$
  L = 
  \frac{\alpha}{2} \left(1 - \frac{\alpha}{2}\right)
  \left(
    \frac{Z_{\frac{1-s}{2}}}{\epsilon \pi(\theta_p)}
  \right)^2
$$
To further illustrate this, @tbl-n_post_draws shows the number of posterior draws reqired to accurately estimate the bounds of a 99% and 95% credible interval within 1% of the true bounds with probability 0.99. In all cases, the number of posterior draws required is in the hundreds of thousands.  However, the default settings for most existing popular R packages for Bayesian analysis are drastically smaller (e.g., both `rstanarm` and `brms` have a default of 4000). 
```{r}
#| label: tbl-n_post_draws
#| tbl-cap: "Number of posterior draws to estimate CI bounds for common distributions"

# Get 1% of true quantile
relative_epsilon = 0.01

# Obtain this with probability 0.99
s = 0.99

# True quantiles of interest (for 99% and 95% CIs)
tail_probabilities = c(0.005,0.025,0.975,0.995)

# Get the true quantiles
true_quantiles = 
  rbind(
    qnorm(tail_probabilities),
    qt(tail_probabilities,
       df = 5),
    qgamma(tail_probabilities,
           shape = 10,
           rate = 1),
    qbeta(tail_probabilities,
          shape1 = 5,
          shape2 = 2))

# Get PDF values
pdf_values = 
  rbind(
    dnorm(true_quantiles[1,]),
    dt(true_quantiles[2,],
       df = 5),
    dgamma(true_quantiles[3,],
           shape = 10,
           rate = 1),
    dbeta(true_quantiles[4,],
          shape1 = 5,
          shape2 = 2))

# Move from relative to absolute error
absolute_epsilon = 
  abs(true_quantiles * relative_epsilon)

# Get the number of posterior draws needed
n_posterior_draws_needed = 
  ceiling(
    matrix(tail_probabilities * (1.0 - tail_probabilities),
           4,4,byrow = T) * 
    (
      qnorm(0.5 * (1.0 - s)) / 
        ( absolute_epsilon * pdf_values)
    )^2
  )
rownames(n_posterior_draws_needed) = 
  c("N(0,1)",
    "t(5)",
    "Gamma(10,1)",
    "Beta(5,2)")
colnames(n_posterior_draws_needed) = 
  paste(100 * tail_probabilities,
        "%",
        sep = "")

knitr::kable(n_posterior_draws_needed,
             booktabs = TRUE)
```


In practice, `bayesics` uses a small number of draws to estimate $\theta_p$ which is then used to compute the total number of posterior draws needed.  While these numbers are much larger than the number of samples existing packages tend to give by default, the functions in `bayesics` are still very fast since the samples are done independently using already optimized functions (e.g., from the `mvtnorm` package [@genz2009mvtnorm]), and can be obtained through embarrassingly parallel processes, which `bayesics` has set up to work with the `future` package [@bengtsson2021future]


# Getting started
As a quick introduction on using the `bayesics` package, we will recreate the example in the help documentation of `lm()`.  Here are the data
```{r}
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm(weight ~ group)
```

To fit a linear regression model using Bayesian methods, it is just as simple:
```{r}
library(bayesics)
lm_b(weight ~ group)
```

We can generate our own data for an example where we know the truth.  Consider the following setup:
$$
y_i = -1 + x_{i1} + 2\cdot1_{[x_3 \in\{d,e\}]} + N(0,1).
$$

And here are the data:
```{r}
set.seed(2025)
N = 500
test_data = 
  data.frame(x1 = rnorm(N),
             x2 = rnorm(N),
             x3 = letters[1:5])
test_data$outcome = 
  rnorm(N,-1 + test_data$x1 + 2 * (test_data$x3 %in% c("d","e")) )
```

We can fit it similarly to above, but using a Zellner's g prior:
```{r}
test_fit = 
  lm_b(outcome ~ x1 + x2 + x3,
       data = test_data,
       prior = "zellner")
summary(test_fit,
        CI_level = 0.99)
```
Note that `summary.lm_b` provides the point estimates (the posterior means), the 99% credible intervals, the probability of directions, the posterior probability that the regression coefficient is in the ROPE (and what the ROPE equals), the Bayes factor in favor of the alternative (i.e., that $\beta_j\neq 0$), and Kass and Raftery's interpretation.  

We can also obtain diagnostic plots, partial dependence plots (typically only interesting if there are interaction terms), credible interval bands, and prediction interval bands.  We can obtain any combination of these.  In the above example since we do not have any interaction terms, we can skip the PDPs.  When plotting credible or prediction interval bands, the user can either supply an exemplar covariate (i.e., the values of other covariates held constant as the covariate of interest is changed), or else plot.lm_b will find the medoid observation and use that observation's covariates as the exemplar.
```{r}
plot(test_fit,
     type = c("dx","ci","pi"))
```


# Functionality

Below, @tbl-functionality describes the functionality of `bayesics`.  Note that `*IC` indicates AIC, BIC, DIC, and WAIC.  Three of these functions act as wrappers for functions from the `DFBA` package [@barch2023dfba], ensuring common interfacing as other `bayesics` functions, and adding ROPE and probability of direction.


Table: Functionality of the `bayesics` R package {#tbl-functionality}

| Function | Analysis | Generics | Wrapper for... |
|----------|----------|----------|----------------|
| `aov_b`  | 1-way analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` <br> `get_posterior_draws` <br> `*IC` | |
| `bma_inference`  | Bayesian model averaging for linear regression models | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | |
| `case_control_b`  | Analysis of 2x2 contingency tables from a case-control study |  | |
| `homogeneity_b`  | Test of homogeneity for 2-way contingency tables |  | |
| `independence_b`  | Test of independence for 2-way contingency tables |  | |
| `cor_test_b`  | Analysis of Kendall's tau correlation coefficient |  | `DFBA::dfba_bivariate_concordance` |
| `find_beta_parms`  | Find shape parameters of the Beta distribution which tries to match a user-specified mean and quantile |  | |
| `find_invgamma_parms`  | Find shape and rate parameters of the Inverse Gamma distribution that tries to match user-specified median and one other quantile | | |
| `glm_b`  | Generalized linear models | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` <br> `get_posterior_draws` <br> `*IC` | |
| `heteroscedasticity_test`  | Test whether two or more populations have equal variances | | |
| `lm_b`  | Linear models | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` <br> `get_posterior_draws` <br> `*IC` | |
| `mediate_b`  | Mediation analysis using the framework of [@imai2010general] |  | |
| `np_glm_b`  | Generalized Bayesian inference via the loss-likelihood bootstrap | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | |
| `poisson_test_b`  | Make inference on one or two populations using Poisson distributed count data |  | |
| `prop_test_b`  | Make inference on a single population proportion or compare two population proportions |  | |
| `aov_b`  | Analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | |
| `SDratio`  | Use the Savage-Dickey ratio to compute Bayes factors on `lm_b` or `glm_b`|  | |
| `sign_test_b`  | Sign test for paired data |  | |
| `t_test_b`  | Make inference on one or two population means using normally distributed data | | |
| `wilcoxon_test_b`  | Wilcoxon Rank Sum (aka Mann-Whitney U) or the Wilcoxon signed rank analyses | | `DFBA::dfba_mann_whitney` <br> `DFBA::dfba_wilcoxon` |
<!-- | `aov_b`  | Analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | | -->
<!-- | `aov_b`  | Analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | | -->
<!-- | `aov_b`  | Analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | | -->
<!-- | `aov_b`  | Analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | | -->
<!-- | `aov_b`  | Analysis of variance | `coef` <br> `plot` <br> `predict` <br> `print` <br> `summary` | | -->










